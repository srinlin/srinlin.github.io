---
title: week6_mini_project
date: 2022-06-16T13:59:00.723Z

categories:
  - DataScience
tags:
  - DataScience
  - pandas
  - seaborn
  - numpy
  - Titanic
  - ML
  - classfication
  - project
toc_label: 미니 프로젝트 페이지
---
6주차 미니프로젝트 머신러닝 classification

## 필요한 라이브러리 및 데이터 불러오기


```python
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```


```python
df = sns.load_dataset("titanic")
```

## 데이터셋 EDA 및 전처리


```python
df.shape
```




    (891, 15)




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 15 columns):
     #   Column       Non-Null Count  Dtype   
    ---  ------       --------------  -----   
     0   survived     891 non-null    int64   
     1   pclass       891 non-null    int64   
     2   sex          891 non-null    object  
     3   age          714 non-null    float64 
     4   sibsp        891 non-null    int64   
     5   parch        891 non-null    int64   
     6   fare         891 non-null    float64 
     7   embarked     889 non-null    object  
     8   class        891 non-null    category
     9   who          891 non-null    object  
     10  adult_male   891 non-null    bool    
     11  deck         203 non-null    category
     12  embark_town  889 non-null    object  
     13  alive        891 non-null    object  
     14  alone        891 non-null    bool    
    dtypes: bool(2), category(2), float64(2), int64(4), object(5)
    memory usage: 80.7+ KB



```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
      <th>class</th>
      <th>who</th>
      <th>adult_male</th>
      <th>deck</th>
      <th>embark_town</th>
      <th>alive</th>
      <th>alone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>yes</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>S</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>714.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.383838</td>
      <td>2.308642</td>
      <td>29.699118</td>
      <td>0.523008</td>
      <td>0.381594</td>
      <td>32.204208</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.486592</td>
      <td>0.836071</td>
      <td>14.526497</td>
      <td>1.102743</td>
      <td>0.806057</td>
      <td>49.693429</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.125000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.910400</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
    </tr>
  </tbody>
</table>
</div>



### 결측치 처리하기

* age랑 deck 변수안에 결측치가 꽤나 있는 것을 확인하였다. embark_town에도 조금 있는 것을 확인할 수 있었다.
* deck 이라는 변수안에는 결측치가 너무 많으므로 추후에 drop 시킨다.
* age의 경우 따로 나이를 설정해줄 수도 있으나 임의로 나이를 설정하기에는 결과에 영향을 많이 줄 것같으므로 나이에 결측치가 있는 행을 삭제해준다.
* embark_town의 경우도 위와 같이 실행한다.


```python
df.isnull().sum()
```




    survived         0
    pclass           0
    sex              0
    age            177
    sibsp            0
    parch            0
    fare             0
    embarked         2
    class            0
    who              0
    adult_male       0
    deck           688
    embark_town      2
    alive            0
    alone            0
    dtype: int64




```python
# deck 이라는 변수 드랍 시키기
df=df.drop("deck", axis=1)
```


```python
# 결측치가 들어있는 행 삭제 시키기
df=df.dropna()
```

### 필요없는 칼럼 삭제하기

메모리를 효율적으로 사용하기 위해
* alive와 survived는 같은 항목이므로 alive 칼럼을 drop
* sex와 who 칼럼으로 성인 남성이 이미 구분되므로 adult_male drop  
시킨다


```python
df=df.drop('alive', axis=1)
df=df.drop('adult_male', axis=1)
df=df.drop("class", axis=1)
```

### string으로 구성된 칼럼 변경하기

string 값으로 들어가 있는 칼럼들을 다음과 같이 바꿔준다.


```python
df['sex'].value_counts()
```




    male      453
    female    259
    Name: sex, dtype: int64




```python
df['embarked'].value_counts()
```




    S    554
    C    130
    Q     28
    Name: embarked, dtype: int64




```python
df['who'].value_counts()
```




    man      413
    woman    216
    child     83
    Name: who, dtype: int64




```python
df['embark_town'].value_counts()
```




    Southampton    554
    Cherbourg      130
    Queenstown      28
    Name: embark_town, dtype: int64




```python
def change_to_index(columns):
    tmp = {string : i for i,string in enumerate(df[columns].unique())}
    df[columns] = df[columns].map(tmp)
    print(tmp)
```


```python
change_list = ['who', 'sex', 'embarked', 'embark_town']
for col in change_list:
    change_to_index(col)
```

    {'man': 0, 'woman': 1, 'child': 2}
    {'male': 0, 'female': 1}
    {'S': 0, 'C': 1, 'Q': 2}
    {'Southampton': 0, 'Cherbourg': 1, 'Queenstown': 2}


여기 부분 함수로 바꿀 수 있을 거 같습니다

### 시각화를 통해 데이터 확인하기

* 결정 변수 안의 값 확인하기


```python
df['survived'].value_counts()
```




    0    424
    1    288
    Name: survived, dtype: int64



* 전체 데이터에 대한 히스토그램 그려보기


```python
_=df.hist(bins=50, figsize=(12,8))
```


    
![png](/assets/images/source_images/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_files/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_29_0.png)
    


* 변수들간의 상관관계 분석하기


```python
df.corr()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
      <th>who</th>
      <th>embark_town</th>
      <th>alone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>survived</th>
      <td>1.000000</td>
      <td>-0.356462</td>
      <td>0.536762</td>
      <td>-0.082446</td>
      <td>-0.015523</td>
      <td>0.095265</td>
      <td>0.266100</td>
      <td>0.108517</td>
      <td>0.455489</td>
      <td>0.108517</td>
      <td>-0.199741</td>
    </tr>
    <tr>
      <th>pclass</th>
      <td>-0.356462</td>
      <td>1.000000</td>
      <td>-0.150826</td>
      <td>-0.365902</td>
      <td>0.065187</td>
      <td>0.023666</td>
      <td>-0.552893</td>
      <td>-0.108502</td>
      <td>0.010297</td>
      <td>-0.108502</td>
      <td>0.150576</td>
    </tr>
    <tr>
      <th>sex</th>
      <td>0.536762</td>
      <td>-0.150826</td>
      <td>1.000000</td>
      <td>-0.099037</td>
      <td>0.106296</td>
      <td>0.249543</td>
      <td>0.182457</td>
      <td>0.097129</td>
      <td>0.685744</td>
      <td>0.097129</td>
      <td>-0.284009</td>
    </tr>
    <tr>
      <th>age</th>
      <td>-0.082446</td>
      <td>-0.365902</td>
      <td>-0.099037</td>
      <td>1.000000</td>
      <td>-0.307351</td>
      <td>-0.187896</td>
      <td>0.093143</td>
      <td>0.012186</td>
      <td>-0.473611</td>
      <td>0.012186</td>
      <td>0.195766</td>
    </tr>
    <tr>
      <th>sibsp</th>
      <td>-0.015523</td>
      <td>0.065187</td>
      <td>0.106296</td>
      <td>-0.307351</td>
      <td>1.000000</td>
      <td>0.383338</td>
      <td>0.139860</td>
      <td>0.004021</td>
      <td>0.443029</td>
      <td>0.004021</td>
      <td>-0.629408</td>
    </tr>
    <tr>
      <th>parch</th>
      <td>0.095265</td>
      <td>0.023666</td>
      <td>0.249543</td>
      <td>-0.187896</td>
      <td>0.383338</td>
      <td>1.000000</td>
      <td>0.206624</td>
      <td>-0.014082</td>
      <td>0.423730</td>
      <td>-0.014082</td>
      <td>-0.577109</td>
    </tr>
    <tr>
      <th>fare</th>
      <td>0.266100</td>
      <td>-0.552893</td>
      <td>0.182457</td>
      <td>0.093143</td>
      <td>0.139860</td>
      <td>0.206624</td>
      <td>1.000000</td>
      <td>0.176859</td>
      <td>0.120515</td>
      <td>0.176859</td>
      <td>-0.262799</td>
    </tr>
    <tr>
      <th>embarked</th>
      <td>0.108517</td>
      <td>-0.108502</td>
      <td>0.097129</td>
      <td>0.012186</td>
      <td>0.004021</td>
      <td>-0.014082</td>
      <td>0.176859</td>
      <td>1.000000</td>
      <td>0.101745</td>
      <td>1.000000</td>
      <td>-0.049007</td>
    </tr>
    <tr>
      <th>who</th>
      <td>0.455489</td>
      <td>0.010297</td>
      <td>0.685744</td>
      <td>-0.473611</td>
      <td>0.443029</td>
      <td>0.423730</td>
      <td>0.120515</td>
      <td>0.101745</td>
      <td>1.000000</td>
      <td>0.101745</td>
      <td>-0.451685</td>
    </tr>
    <tr>
      <th>embark_town</th>
      <td>0.108517</td>
      <td>-0.108502</td>
      <td>0.097129</td>
      <td>0.012186</td>
      <td>0.004021</td>
      <td>-0.014082</td>
      <td>0.176859</td>
      <td>1.000000</td>
      <td>0.101745</td>
      <td>1.000000</td>
      <td>-0.049007</td>
    </tr>
    <tr>
      <th>alone</th>
      <td>-0.199741</td>
      <td>0.150576</td>
      <td>-0.284009</td>
      <td>0.195766</td>
      <td>-0.629408</td>
      <td>-0.577109</td>
      <td>-0.262799</td>
      <td>-0.049007</td>
      <td>-0.451685</td>
      <td>-0.049007</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



가장 눈에 먼저 들어오는 것은 검정부분들인데,
  - (sex, survived)
  - (pclass, fare)
  - (sibsp, alone)
  - (parch, alone)  
를 대표적으로 뽑아 볼 수 있다.


```python
_=sns.heatmap(data=df.corr())
```


    
![png](/assets/images/source_images/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_files/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_33_0.png)
    



```python
df.isnull().sum()
```




    survived       0
    pclass         0
    sex            0
    age            0
    sibsp          0
    parch          0
    fare           0
    embarked       0
    who            0
    embark_town    0
    alone          0
    dtype: int64



## 의사결정나무 

EDA는 우선 여기까지 하고 더 보충하면 될 거 같습니다.

### 학습, 예측을 위한 사전 준비


```python
# 종속변수 이름 설정하기
label_name='survived'
```


```python
# 독립변수 이름 리스트 설정하기
feature_names=df.columns.tolist()
feature_names.remove(label_name)
feature_names
```




    ['pclass',
     'sex',
     'age',
     'sibsp',
     'parch',
     'fare',
     'embarked',
     'who',
     'embark_town',
     'alone']




```python
# 각 X와 y 만들어주기
X=df[feature_names]
y=df[label_name]
X.shape, y.shape
```




    ((712, 10), (712,))



### training과 test 데이터셋 만들기


```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
```


```python
X_train, X_test, y_train, y_test = train_test_split(X,
                                                   y,
                                                   test_size=0.2,
                                                   stratify=y,
                                                   random_state=42)
```

* 아래의 결과로 보아 잘 만들어 졌음을 확인할 수 있다.


```python
X_train.shape,y_train.shape, X_test.shape, y_test.shape
```




    ((569, 10), (569,), (143, 10), (143,))



### 의사결정나무 모델 구축하기(하이퍼 파라미터 튜닝X)

우선 하이퍼파라미터 따로 튜닝 하지 않고,
* max_depth=15
* max_features=0.9
* random_state=42  
를 이용하여 의사결정나무 모델을 구축해보도록 하겠다

**알고리즘 가져오기**


```python
model = DecisionTreeClassifier(max_depth=10, 
                               max_features=0.9, 
                               random_state=42)
model
```




    DecisionTreeClassifier(max_depth=10, max_features=0.9, random_state=42)



**학습시키기**


```python
model.fit(X_train, y_train)
```




    DecisionTreeClassifier(max_depth=10, max_features=0.9, random_state=42)




```python
y_predict = model.predict(X_test)
y_predict[:10]
```




    array([1, 0, 0, 0, 0, 0, 0, 1, 0, 1], dtype=int64)




```python
plt.figure(figsize=(20, 20))
plot_tree(model, filled=True, fontsize=14, feature_names=feature_names)
plt.show()
```


    
![png](/assets/images/source_images/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_files/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_53_0.png)
    


**Feature 중요도 확인하기**


```python
sns.barplot(x=model.feature_importances_, y=feature_names)
```




    <AxesSubplot:>




    
![png](/assets/images/source_images/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_files/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_55_1.png)
    


위의 결과를 통해 who의 중요도가 가장 높게 나왔고, embark_town의 중요도가 가장 낮은 것으로 나오는것을 확인할 수 있었다.

**정확도 측정하기**


```python
accuracy_score(y_test, y_predict)
```




    0.7132867132867133



정확도는 대략 78% 정도가 나왔음을 확인할 수 있다.

### 의사결정나무 모델 구축하기(GridSearchCV)


```python
# 각 파라미터에 대한 리스트를 만들어주기
max_depth_list = [2,3,5,7,9,12,15,20,40]
max_features_list =[0.1,0.2, 0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
```

**학습시키기**


```python
#GridSearchCV에 적용하기
parameters = {'max_depth':max_depth_list, 'max_features':max_features_list}

clf=GridSearchCV(model, param_grid=parameters, scoring="accuracy", n_jobs=-1, cv=5)
clf.fit(X_train,y_train)
```




    GridSearchCV(cv=5,
                 estimator=DecisionTreeClassifier(max_depth=10, max_features=0.9,
                                                  random_state=42),
                 n_jobs=-1,
                 param_grid={'max_depth': [2, 3, 5, 7, 9, 12, 15, 20, 40],
                             'max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
                                              0.8, 0.9, 1]},
                 scoring='accuracy')



* GridSearchCV를 이용하여 모델을 돌려본 결과 다음과 같은 파라미터에서 다음과 같은결과가 가장 높은 결과라는 것을 알 수 있었다.
* 확실히 파라미터를 임의로 선택하였을 때보다 더 높은 정확도를 보인다는 것을 확인할 수 있다.


```python
clf.best_estimator_
```




    DecisionTreeClassifier(max_depth=3, max_features=0.7, random_state=42)




```python
# 최고 결과
clf.best_score_
```




    0.8382394038192829



**Feature 중요도 확인하기**

who의 중요도가 더 상승하였고, pclass, age, fare 순으로 중요도를 가지고 있음을 확인할 수 있었다.


```python
best_model_grid=clf.best_estimator_
best_model_grid.fit(X_train,y_train)
```




    DecisionTreeClassifier(max_depth=3, max_features=0.7, random_state=42)




```python
sns.barplot(x=best_model_grid.feature_importances_,y=feature_names)
```




    <AxesSubplot:>




    
![png](/assets/images/source_images/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_files/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_70_1.png)
    


### 의사결정나무 모델 구축하기(RandomSearchCV)


```python
max_depth_list = [2,3,5,7,9,12,15,20,40]
max_features_list =[0.1,0.2, 0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
```

**학습시키기**


```python
param_distributions={"max_depth": max_depth_list, "max_features":max_features_list }

clf=RandomizedSearchCV(model, param_distributions,cv=5,n_jobs=-1,random_state=42,n_iter=5)

clf.fit(X_train,y_train)
```




    RandomizedSearchCV(cv=5,
                       estimator=DecisionTreeClassifier(max_depth=10,
                                                        max_features=0.9,
                                                        random_state=42),
                       n_iter=5, n_jobs=-1,
                       param_distributions={'max_depth': [2, 3, 5, 7, 9, 12, 15, 20,
                                                          40],
                                            'max_features': [0.1, 0.2, 0.3, 0.4,
                                                             0.5, 0.6, 0.7, 0.8,
                                                             0.9, 1]},
                       random_state=42)



같은 조건에서
* RandSearchCV를 이용하여 max_depth=5, max_features=0.3 일때 가장 좋은 결과를 얻을 수 있었다.
* GridSearchCV를 이용하였을 때보단 정확도는 더 떨어진 모습을 보이고 있다.


```python
clf.best_estimator_
```




    DecisionTreeClassifier(max_depth=5, max_features=0.3, random_state=42)




```python
#최고 결과
clf.best_score_
```




    0.8137401024685609



**Feature 중요도 확인하기**

위와 같이 who의 중요도가 가장 높았고 pclass, age, fare 순으로 중요도가 높았다


```python
best_model_rand=clf.best_estimator_
best_model_rand.fit(X_train,y_train)
```




    DecisionTreeClassifier(max_depth=5, max_features=0.3, random_state=42)




```python
sns.barplot(x=best_model_rand.feature_importances_,y=feature_names)
```




    <AxesSubplot:>




    
![png](/assets/images/source_images/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_files/8%EC%A1%B0_6%EC%A3%BC%EC%B0%A8_%EB%AF%B8%EB%8B%88%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_81_1.png)
    


### 의사결정나무 모델 구축하기(GridSearchCV, RandSearchCV) 

max_feature을 np.random으로 파라미터 리스트 만들어서 진행하기


```python
max_depth_list=[i for i in range(1,11)]
max_features_list=np.random.uniform(0.3,0.9,20)
```


```python
max_features_list
```




    array([0.60160975, 0.3864401 , 0.57485657, 0.55484182, 0.86127543,
           0.46827385, 0.47372652, 0.53613517, 0.81719125, 0.43946852,
           0.69586836, 0.53851674, 0.59967992, 0.32745853, 0.42479905,
           0.39089208, 0.39700768, 0.75139024, 0.51406105, 0.53080799])



**함수로 만들어서 한번에 비교하기**

매번 넣어주는 리스트의 구성마다 매번 다를 거기 때문에 함수로 만들어서 진행하기


```python
def compare_GR(depth,feature):
    #gridsearch
    import time
    start = time.time()
    parameters = {'max_depth':max_depth_list, 'max_features':max_features_list}
    clf=GridSearchCV(model, param_grid=parameters, scoring="accuracy", n_jobs=-1, cv=5)
    clf.fit(X_train,y_train)
    grid_score=clf.best_score_
    print(f'GridSearch\n')
    print(f'Best Score parameters: {clf.best_estimator_}\n')
    print(f'Best Score: {grid_score}\n')
    print(f'Time: {time.time() - start}\n')
    print('-------------------------------------------------------------\n')
    
    
    #randsearch
    start = time.time()
    param_distributions={"max_depth": max_depth_list, "max_features":max_features_list }
    clf=RandomizedSearchCV(model, param_distributions,cv=5,n_jobs=-1,random_state=42,n_iter=5)
    clf.fit(X_train,y_train)
    rand_score=clf.best_score_
    print(f'RandSearch\n')
    print(f'Best Score parameters: {clf.best_estimator_}\n')
    print(f'Best Score: {rand_score}\n')
    print(f'Time: {time.time() - start}\n')
    print('-------------------------------------------------------------\n')
    
    #비교하기
    if grid_score > rand_score:
        print('GridSearhcv의 정확도가 더 높습니다.')
    elif grid_score < rand_score:
        print('RandSearhcv의 정확도가 더 높습니다.')
    elif grid_score==rand_score:
        print('같습니다')
    else:
        print('뭔가 이상합니다')
```

## 결론


```python
compare_GR(max_depth_list,max_features_list)
```

    GridSearch
    
    Best Score parameters: DecisionTreeClassifier(max_depth=3, max_features=0.7513902397624934,
                           random_state=42)
    
    Best Score: 0.8382394038192829
    
    Time: 0.8767976760864258
    
    -------------------------------------------------------------
    
    RandSearch
    
    Best Score parameters: DecisionTreeClassifier(max_depth=5, max_features=0.39089207858316505,
                           random_state=42)
    
    Best Score: 0.8137401024685609
    
    Time: 0.04804348945617676
    
    -------------------------------------------------------------
    
    GridSearhcv의 정확도가 더 높습니다.

